{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family:consolas\"> 리플레이 메모리 <br> 리스트에다가 데이터셋 [인덱스, 중요도, 이전 상태, 행위, 보상, 끝 여부, 이후 상태] 를 저장하고, <br> 세그먼트 트리로 중요도에 비례하는 뽑힐 확률을 가지게끔 함.(지난 게시물 참고) <br><br> 그리고 이건 정석적인 방식은 아닌데, <br>중요도만 따로 저장하고 있는 ndarray를 만들어서, 리플레이 메모리가 꽉 차서 새 걸 넣으려면 하나를 지워야 할 때,<br> np.argmin으로 가장 중요도가 낮은 놈을 찾아서 지우게끔 해봤음. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_lib=ctypes.cdll.LoadLibrary(\"C:/Users/love4/Desktop/pr/c/segtree.dll\")\n",
    "\n",
    "class segtree(object):\n",
    "    def __init__(self, maxlen):\n",
    "        tree_lib.tree_edit.argtypes=[ctypes.c_void_p, ctypes.c_int, ctypes.c_double]\n",
    "        tree_lib.tree_get_idx.argtypes=[ctypes.c_void_p, ctypes.c_double]\n",
    "        tree_lib.total.argtypes=[ctypes.c_void_p]\n",
    "        tree_lib.tree_get_idx.restype=ctypes.c_int\n",
    "        tree_lib.new_tree.restype=ctypes.c_void_p\n",
    "        tree_lib.total.restype=ctypes.c_double\n",
    "        self.tree=ctypes.c_void_p(tree_lib.new_tree(maxlen))\n",
    "    def edit_tree(self, idx, val):\n",
    "        tree_lib.tree_edit(self.tree, idx, ctypes.c_double(val))\n",
    "    def get_idx(self, goal):\n",
    "        return tree_lib.tree_get_idx(self.tree, ctypes.c_double(goal))\n",
    "    def total(self):\n",
    "        return tree_lib.total(self.tree)\n",
    "\n",
    "class replaymem:\n",
    "    def __init__(self, maxlen):\n",
    "        self.tree=segtree(maxlen)\n",
    "        self.memory=[None for _ in range(maxlen+1)]\n",
    "        self.priority_array=np.zeros(maxlen+1, dtype=np.float32)\n",
    "        self.len=0\n",
    "        self.maxlen=maxlen\n",
    "\n",
    "    def push(self, priority:float, i_state:np.ndarray, action:int, reward:float, done:int, f_state:np.ndarray):\n",
    "        if self.len<self.maxlen:\n",
    "            self.tree.edit_tree(self.len+1, priority)\n",
    "            self.memory[self.len+1]=[self.len+1, priority, i_state, action, reward, done, f_state]\n",
    "        else:\n",
    "            edit_idx=np.argmin(self.priority_array[1:])+1\n",
    "            self.tree.edit_tree(edit_idx, priority-self.memory[edit_idx][1])\n",
    "            self.memory[edit_idx]=[edit_idx, priority, i_state, action, reward, done, f_state]\n",
    "        self.len=min(self.len+1, self.maxlen)\n",
    "        \n",
    "\n",
    "    def edit_prior(self, idx, priority:float):\n",
    "        self.tree.edit_tree(idx, priority-self.memory[idx][1])\n",
    "        self.memory[idx][1]=priority\n",
    "        self.priority_array[idx]=priority\n",
    "\n",
    "    def sample(self, size):\n",
    "        total=self.tree.total()\n",
    "        for __ in range(size):\n",
    "            yield self.memory[self.tree.get_idx(random.uniform(0, total))]\n",
    "    \n",
    "    def total(self):\n",
    "        return self.tree.total()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family:consolas\"> 유사 Dueling DQN <br> 모델의 성능은 그닥임. 다만 이리저리 바꿔본 결과 연산속도 대비 정확도는 요게 제일 괜찮았음. <br> 기회가 되면 CNN으로도 구현해 볼 예정. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.advantage=nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 4)\n",
    "        )\n",
    "        self.value=nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x=self.net(x)\n",
    "        return self.advantage(x)+self.value(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family:consolas\"> 중요도가 높은 기억일수록 자주 떠오를텐데, <br>샘플의 weights를 계산하여 그것이 학습 과정에 어느 정도 영향을 미칠지 beta 변수를 이용해 조절할 수 있음.<br> (어제 올린 도태남 나오는 게시물 참고) <br> 각 중요도들에 alpha 라는 변수를 제곱하는 과정도 있는데 이건 구현 못함. <br> 알고리즘은 주석으로 자세히 비유해서 설명해 놓음 <span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # 하이퍼 파라미터들 정의\n",
    "        self.epsilon=1 # 랜덤 행동 할 확률\n",
    "        self.gamma=.97 # 감가율 (빙빙 도는걸 방지)\n",
    "        self.eps_decay=.999 # epsilon에다가 이걸 곱해서 epsilon을 조금씩 줄임\n",
    "        self.alpha=1\n",
    "        self.beta=.4\n",
    "        self.e=0.001 # 드물겠지만 중요도가 0으로 업데이트 되는 기억이 발생하는걸 방지\n",
    "        self.alpha_increase=.001\n",
    "        self.beta_increase=.00001 \n",
    "        # beta를 조금씩 늘려 나감. 연애 경험이 많아질수록 \n",
    "        # 연애 기억에서 얻는 타격이 적어질 거잖음?\n",
    "        \n",
    "        # 리플레이 메모리\n",
    "        self.replaybuff=replaymem(50000)\n",
    "        \n",
    "        # main_net과 target_net. 이걸 왜 나누는지는 비유할 방법을 생각중\n",
    "        self.main_net=DQN().to('cuda')\n",
    "        self.target_net=DQN().to('cuda')\n",
    "\n",
    "        self.main_net_optimizer=torch.optim.Adam(self.main_net.parameters())\n",
    "        \n",
    "    def decide_action(self, state: np.ndarray) -> int:\n",
    "        # 행위를 정함. epsilon의 확률로 아무 짓이나 함\n",
    "        if random.random()<self.epsilon:\n",
    "            return random.randint(0, 3)\n",
    "        else:\n",
    "            return int(torch.argmax(self.main_net.forward(self._to_cuda_tensor(state))))\n",
    "    \n",
    "    def _to_cuda_tensor(self, x):\n",
    "        # 튜플/리스트로 묶인 기억들을 행렬로 바꾼 다음 \n",
    "        # 그래픽카드에 올려주는 함수\n",
    "        return torch.tensor(x, device='cuda')\n",
    "\n",
    "    def learn(self):\n",
    "        if self.replaybuff.len<256:\n",
    "            return\n",
    "\n",
    "        # 128개의 기억을 떠올림.\n",
    "        idxs, priorities, i_states, actions, rewards, dones, f_states=zip(*self.replaybuff.sample(128))\n",
    "        priorities=self._to_cuda_tensor(priorities)\n",
    "        rewards=self._to_cuda_tensor(rewards)\n",
    "        dones=torch.tensor(dones, dtype=torch.float32, device='cuda')\n",
    "        i_states=torch.stack(tuple(map(self._to_cuda_tensor, i_states)))\n",
    "        f_states=torch.stack(tuple(map(self._to_cuda_tensor, f_states)))\n",
    "\n",
    "        # DDQN을 적용한건데 이건 비유할 방법을 아직 못 찾음. \n",
    "        # 암튼 궁극적인 목표는 td_targets와 preds가 모든 상황에 대해 같게 만드는거임.\n",
    "        td_targets=rewards+self.gamma*self.target_net.forward(f_states)[np.arange(128), torch.argmax(self.main_net.forward(f_states), dim=1)]*(1-dones)\n",
    "        preds=self.main_net.forward(i_states)[np.arange(128), actions]\n",
    "\n",
    "        # |td_targets-preds|에 비례하게 각 기억들의 중요도를 업데이트 해줌.\n",
    "        # 내 예상과 크게 다른 일이 있었던 기억이 더 잘 떠올라야 하잖음?\n",
    "        # weight를 계산하는 과정은 도태남 게시물 참조\n",
    "        td_errors=td_targets-preds\n",
    "        weights=(self.replaybuff.len*priorities/self.replaybuff.total())**(-self.beta)\n",
    "        weights/=torch.max(weights)\n",
    "        \n",
    "        for i in range(128):\n",
    "            self.replaybuff.edit_prior(idxs[i], float(torch.abs(td_errors[i]))**self.alpha+self.e)        \n",
    "        \n",
    "        # td_targets 와 preds 의 오차를 이용해 main_net의 신경망 구조 수정\n",
    "        self.main_net_optimizer.zero_grad()\n",
    "        lossfn=nn.SmoothL1Loss()\n",
    "        loss=(weights*lossfn(preds, td_targets)).mean()\n",
    "        loss.backward()\n",
    "        self.main_net_optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        #target_net에다가 main_net을 복사\n",
    "        self.target_net=deepcopy(self.main_net)\n",
    "        \"\"\" self.epsilon*=self.eps_decay \"\"\"\n",
    "        self.beta=min(self.beta+self.beta_increase, 0.9)\n",
    "        \"\"\" self.alpha=min(self.alpha+self.alpha_increase, 1) \"\"\"\n",
    "        print(self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent()\n",
    "env=gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family:consolas\"> 컴퓨터가 노오오동을 함 </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=0\n",
    "for episode in range(500):\n",
    "    i_state=env.reset()\n",
    "    total_reward=0\n",
    "    while True:\n",
    "        action=agent.decide_action(i_state)\n",
    "        f_state, reward, done, _=env.step(action)\n",
    "        total_reward+=reward\n",
    "        steps+=1\n",
    "        agent.learn()\n",
    "        env.render(mode='rgb_array')\n",
    "        if steps%50==0:\n",
    "            agent.update_target()\n",
    "        agent.replaybuff.push(float(np.max(agent.replaybuff.priority_array))+0.1, np.copy(i_state), action, reward, done, np.copy(f_state))\n",
    "        if done:   \n",
    "            break\n",
    "        else:\n",
    "            i_state=np.copy(f_state)\n",
    "    \"\"\" if episode==100:\n",
    "        breakpoint() \"\"\"\n",
    "    agent.epsilon*=agent.eps_decay\n",
    "\n",
    "torch.save(agent.target_net.state_dict(), \"lunar_target.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.reset()\n",
    "for i in range(1000):\n",
    "    state, reward, done, _=env.step(int(torch.argmax(agent.main_net.forward(torch.tensor(state, device='cuda')))))\n",
    "    env.render(mode='rgb_array')\n",
    "    print(reward)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('love4ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f18ae30d8c66d70b70f9056193cb7f462aa0a1a92f2b7abb2342a9d5cf787112"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
